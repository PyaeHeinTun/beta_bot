{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # noqa\n",
    "import pandas as pd  # noqa\n",
    "from pandas import DataFrame\n",
    "\n",
    "# --------------------------------\n",
    "# Add your lib to import here\n",
    "import talib.abstract as ta\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import math\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler , MinMaxScaler , RobustScaler\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "binance requires to release all resources with an explicit call to the .close() coroutine. If you are using the exchange instance with async coroutines, add `await exchange.close()` to your code into a place when you're done with the exchange and don't need the exchange instance anymore (at the end of your async coroutine).\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x727d6508a750>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x727d2ef0c890>, 26131.798590309)]', '[(<aiohttp.client_proto.ResponseHandler object at 0x727d2ef0c350>, 26131.864894559), (<aiohttp.client_proto.ResponseHandler object at 0x727d2ef0d250>, 26133.357740267)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x727d646252e0>\n",
      "/tmp/ipykernel_150389/3340177743.py:26: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.strptime(str(datetime.utcnow()).split('.')[0],'%Y-%m-%d %H:%M:%S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    date          open          high           low  \\\n",
      "0    2024-10-21 00:14:00  69109.419149  69119.224681  69108.840851   \n",
      "1    2024-10-21 00:15:00  69121.598511  69194.610000  69121.598511   \n",
      "2    2024-10-21 00:16:00  69194.385106  69284.452766  69189.399574   \n",
      "3    2024-10-21 00:17:00  69283.895957  69406.002766  69283.895957   \n",
      "4    2024-10-21 00:18:00  69360.375957  69381.569362  69328.728936   \n",
      "...                  ...           ...           ...           ...   \n",
      "1995 2024-10-22 09:29:00  66862.418085  66864.383830  66860.604468   \n",
      "1996 2024-10-22 09:30:00  66863.453404  66884.623404  66837.713191   \n",
      "1997 2024-10-22 09:31:00  66830.498298  66893.219787  66826.384468   \n",
      "1998 2024-10-22 09:32:00  66892.883191  66892.883191  66803.428936   \n",
      "1999 2024-10-22 09:33:00  66812.640638  66868.542553  66812.640638   \n",
      "\n",
      "             close  volume  \n",
      "0     69119.207021     0.0  \n",
      "1     69194.498085     0.0  \n",
      "2     69284.452766     0.0  \n",
      "3     69365.897872     0.0  \n",
      "4     69374.229149     0.0  \n",
      "...            ...     ...  \n",
      "1995  66863.458936     0.0  \n",
      "1996  66837.713191     0.0  \n",
      "1997  66893.219787     0.0  \n",
      "1998  66812.035106     0.0  \n",
      "1999  66868.542553     0.0  \n",
      "\n",
      "[2000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import ccxt.pro as ccxt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Initialize the exchange\n",
    "exchange = ccxt.binance()  # Example exchange, replace with your desired exchange\n",
    "exchange.set_sandbox_mode(True)\n",
    "\n",
    "exchange.api = \"52a65ac457d28e61c7f76a769754b3bdfe413d7c62e09a8605b55aaa140e9284\"\n",
    "exchange.apiKey = \"52a65ac457d28e61c7f76a769754b3bdfe413d7c62e09a8605b55aaa140e9284\"\n",
    "exchange.secret = \"c86f35f7b8e4f75154ef39c650ff1513657a67799a51ee7429dd4d64939ba386\"\n",
    "leverage = 20\n",
    "exchange.options[\"defaultType\"] = \"future\"\n",
    "# Define symbol and timeframe\n",
    "symbol = 'BTC/USDT:USDT'  # Example symbol\n",
    "timeframe = '1m'  # Example timeframe\n",
    "await exchange.set_leverage(leverage=leverage, symbol=symbol)\n",
    "\n",
    "# Initialize an empty DataFrame to store the candles\n",
    "dataframe = pd.DataFrame()\n",
    "\n",
    "# Fetch 10,000 candles in chunks of 1500 candles\n",
    "num_candles_to_fetch = 2000\n",
    "candles_per_request = 1000\n",
    "start_index = 0\n",
    "current_time = datetime.strptime(str(datetime.utcnow()).split('.')[0],'%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "markets = await exchange.load_markets()\n",
    "tick_size = float(markets[symbol]['info']['filters'][0]['tickSize'])\n",
    "while start_index < num_candles_to_fetch:\n",
    "    # Determine the end index for this request\n",
    "    end_index = min(start_index + candles_per_request, num_candles_to_fetch)\n",
    "\n",
    "    # minute_data = (num_candles_to_fetch / end_index)*candles_per_request\n",
    "    minute_data = num_candles_to_fetch - start_index\n",
    "\n",
    "    timeframe_int = int(re.search(r'\\d+', str(timeframe)).group())\n",
    "    previous_time_utc = str(current_time - timedelta(minutes=minute_data*timeframe_int))\n",
    "    formatted_datetime_str = previous_time_utc.split('.')[0]\n",
    "    since = exchange.parse8601(formatted_datetime_str)\n",
    "    # round(datetime.strptime(formatted_datetime_str, '%Y-%m-%d %H:%M:%S').timestamp())\n",
    "    # if (end_index == (num_candles_to_fetch-candles_per_request)) else None\n",
    "    # Fetch candles for this chunk\n",
    "    candles = await exchange.fetch_ohlcv(\n",
    "        symbol,\n",
    "        timeframe, \n",
    "        limit=candles_per_request, \n",
    "        since=since,\n",
    "        params={'price': 'mark'}\n",
    "    )\n",
    "    \n",
    "    # Convert candles to DataFrame\n",
    "    candles_df = pd.DataFrame(candles, columns=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    candles_df['date'] = pd.to_datetime(candles_df['date'], unit='ms')\n",
    "    # Append the candles to the DataFrame\n",
    "    dataframe = pd.concat([dataframe, candles_df], ignore_index=True)\n",
    "    \n",
    "    # # Update start index for the next request\n",
    "    start_index += candles_per_request\n",
    "\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_CCI(dataframe,index,diff,ma,p):\n",
    "    close_array = dataframe['open'].to_numpy()\n",
    "    close_array = close_array[:index.name]\n",
    "    diff = diff.to_numpy()\n",
    "    diff = diff[index.name]\n",
    "    ma = ma.to_numpy()\n",
    "    ma = ma[index.name]\n",
    "\n",
    "    if(len(close_array) < p-1):\n",
    "        return np.nan\n",
    "\n",
    "    s = 0\n",
    "\n",
    "    for i in range(len(close_array),len(close_array)-p,-1):\n",
    "        s = s + abs(dataframe['open'][i] - ma)\n",
    "    mad = s / p\n",
    "\n",
    "    mcci = diff/mad/0.015\n",
    "    \n",
    "    return mcci\n",
    "\n",
    "def calculate_cci_improved(source1:pd.Series,source2:pd.Series,source3:pd.Series,length) -> pd.Series:\n",
    "    source1:np.ndarray = source1.to_numpy()\n",
    "    source2:np.ndarray = source2.to_numpy()\n",
    "    source3:np.ndarray = source3.to_numpy()\n",
    "    \n",
    "    windows = np.lib.stride_tricks.sliding_window_view(source1, window_shape=(length,))\n",
    "    source3_modify = source3[:, np.newaxis][length-1:]\n",
    "    sums = np.sum(np.abs(windows - source3_modify),axis=1)\n",
    "        \n",
    "    mad = sums / length\n",
    "\n",
    "    mad_series = pd.Series(index=range(len(source1)), dtype=float)\n",
    "    mad_series[-len(mad):] = mad\n",
    "    mad_series = mad_series.to_numpy()\n",
    "    mcci = source2/mad_series/0.015\n",
    "    return mcci\n",
    "\n",
    "\n",
    "def rescale(src, old_min, old_max, new_min, new_max):\n",
    "    return new_min + (new_max - new_min) * (src - old_min) / np.maximum((old_max - old_min), 10e-10)\n",
    "\n",
    "def normalize(value, min_val, max_val,df):\n",
    "    index = value.name\n",
    "    src = pd.Series(df[:index+1])\n",
    "\n",
    "    historic_min = 10e10\n",
    "    historic_max = -10e10\n",
    "\n",
    "    src_filled_min = src.fillna(historic_min)\n",
    "    src_filled_max = src.fillna(historic_max)\n",
    "    historic_min = min(src_filled_min.min(), historic_min) if not pd.isna(src_filled_min.min()) else historic_min\n",
    "    historic_max = max(src_filled_max.max(), historic_max) if not pd.isna(src_filled_max.max()) else historic_max\n",
    "\n",
    "    normalized_src = (min_val + (max_val - min_val) * (src[index] - historic_min)) / max((historic_max - historic_min), 10e-10)\n",
    "    return normalized_src\n",
    "\n",
    "def normalize_optimized(min_val, max_val,source:pd.Series):\n",
    "    source = source.to_numpy()\n",
    "    historic_min = 10e10\n",
    "    historic_max = -10e10\n",
    "    src_filled_min = np.nan_to_num(source,historic_min) #source.fillna(historic_min)\n",
    "    src_filled_max = np.nan_to_num(source,historic_max) #source.fillna(historic_max)\n",
    "    historic_min = np.minimum.accumulate(src_filled_min)\n",
    "    historic_max = np.maximum.accumulate(src_filled_max)\n",
    "    division_value = np.maximum((historic_max - historic_min),10e-10)\n",
    "    normalized_src = (min_val + (max_val - min_val) * (source - historic_min)) / division_value\n",
    "    return normalized_src\n",
    "\n",
    "def n_rsi(src, n1, n2):\n",
    "    rsi = ta.RSI(src,n1)\n",
    "    ema_rsi = ta.EMA(rsi,n2)\n",
    "    return rescale(ema_rsi, 0, 100, 0, 1)\n",
    "\n",
    "def n_cci(dataframe, n1 , n2):\n",
    "    df = dataframe.copy()\n",
    "    source = df['close']\n",
    "\n",
    "    df['mas'] = ta.SMA(source,n1)\n",
    "    df['diffs'] = source - df['mas']\n",
    "\n",
    "    # df['cci'] = df.apply((lambda index: calculate_CCI(df,index,df['diffs'],df['mas'],n1)),axis=1)\n",
    "    df['cci'] = pd.Series(calculate_cci_improved(dataframe['open'],df['diffs'],df['mas'],n1))\n",
    "\n",
    "    df['ema_cci'] = ta.EMA(df['cci'],n2)\n",
    "\n",
    "    # normalized_wt_diff = df.apply((lambda x : normalize(x,0,1,df['ema_cci'])),axis=1)\n",
    "    normalized_wt_diff = pd.Series(normalize_optimized(0,1,df['ema_cci']))\n",
    "    return normalized_wt_diff\n",
    "\n",
    "def n_wt(src, n1, n2):\n",
    "    ema1 = ta.EMA(src, n1)\n",
    "    ema2 = ta.EMA(np.abs(src - ema1), n1)\n",
    "    ci = (src - ema1) / (0.015 * ema2)\n",
    "    wt1 = ta.EMA(ci, n2)\n",
    "    wt2 = ta.SMA(wt1, 4)\n",
    "    diff = wt1 - wt2\n",
    "    # normalized_wt_diff = df.apply((lambda x : normalize(x,0,1,diff)),axis=1)\n",
    "    normalized_wt_diff = pd.Series(normalize_optimized(0,1,pd.Series(diff)))\n",
    "    return normalized_wt_diff\n",
    "\n",
    "def calculate_tr_optimized(high,low,close):\n",
    "    previos_close = np.roll(close,1)\n",
    "\n",
    "    diff_h_n_l = high - low\n",
    "    abs_value_h_n_c = np.abs(high - previos_close)\n",
    "    abs_value_h_n_c[0] = abs(high[0] - 0)\n",
    "    abs_value_l_n_c = np.abs(low - previos_close)\n",
    "    abs_value_l_n_c[0] = abs(low[0] - 0)\n",
    "    tr = np.maximum(np.maximum(diff_h_n_l,abs_value_h_n_c),abs_value_l_n_c)\n",
    "    return tr\n",
    "\n",
    "def calculate_directionalMovementPlus_optimized(high,low):\n",
    "    prev_high = np.roll(high,1)\n",
    "    prev_low = np.roll(low,1)\n",
    "\n",
    "    diff_h_n_ph = high - prev_high\n",
    "    diff_h_n_ph[0] = high[0] - 0\n",
    "    diff_pl_n_l = prev_low - low\n",
    "    diff_pl_n_l[0] = 0 - low[0]\n",
    "    dmp_value = np.maximum(diff_h_n_ph, 0) * (diff_h_n_ph > diff_pl_n_l)\n",
    "    return dmp_value\n",
    "\n",
    "def calculate_negMovement_optimized(high,low):\n",
    "    prev_high = np.roll(high,1)\n",
    "    prev_low = np.roll(low,1)\n",
    "\n",
    "    diff_h_n_ph = high - prev_high\n",
    "    diff_h_n_ph[0] = high[0] - 0\n",
    "    diff_pl_n_l = prev_low - low\n",
    "    diff_pl_n_l[0] = 0 - low[0]\n",
    "    negMovement = np.maximum(diff_pl_n_l, 0) * (diff_pl_n_l > diff_h_n_ph)\n",
    "    return negMovement\n",
    "\n",
    "def n_adx_optimized(highSrc:pd.Series, lowSrc:pd.Series, closeSrc:pd.Series, n1:int):\n",
    "    length = n1\n",
    "    highSrc_numpy = highSrc.to_numpy()\n",
    "    lowSrc_numpy = lowSrc.to_numpy()\n",
    "    closeSrc_numpy = closeSrc.to_numpy()\n",
    "    \n",
    "    tr = calculate_tr_optimized(highSrc_numpy,lowSrc_numpy,closeSrc_numpy)\n",
    "    directionalMovementPlus = calculate_directionalMovementPlus_optimized(highSrc_numpy,lowSrc_numpy)\n",
    "    negMovement = calculate_negMovement_optimized(highSrc_numpy,lowSrc_numpy)\n",
    "\n",
    "    trSmooth = np.zeros_like(closeSrc_numpy)\n",
    "    trSmooth[0] = np.nan\n",
    "    for i in range(0, len(tr)):\n",
    "        trSmooth[i] = trSmooth[i-1] - trSmooth[i-1] / length + tr[i]\n",
    "\n",
    "    smoothDirectionalMovementPlus = np.zeros_like(closeSrc)\n",
    "    smoothDirectionalMovementPlus[0] = np.nan\n",
    "    for i in range(0, len(directionalMovementPlus)):\n",
    "        smoothDirectionalMovementPlus[i] = smoothDirectionalMovementPlus[i-1] - smoothDirectionalMovementPlus[i-1] / length + directionalMovementPlus[i]\n",
    "\n",
    "    smoothnegMovement = np.zeros_like(closeSrc)\n",
    "    smoothnegMovement[0] = np.nan\n",
    "    for i in range(0, len(negMovement)):\n",
    "        smoothnegMovement[i] = smoothnegMovement[i-1] - smoothnegMovement[i-1] / length + negMovement[i]\n",
    "\n",
    "    diPositive = smoothDirectionalMovementPlus / trSmooth * 100\n",
    "    diNegative = smoothnegMovement / trSmooth * 100\n",
    "    dx = np.abs(diPositive - diNegative) / (diPositive + diNegative) * 100\n",
    "    dx_series = pd.Series(dx)\n",
    "\n",
    "    adx = dx_series.copy()\n",
    "    adx.iloc[:length] = adx.rolling(length).mean().iloc[:length]\n",
    "    adx = adx.ewm(alpha=(1.0/length),adjust=False).mean()\n",
    "    return rescale(adx, 0, 100, 0, 1)\n",
    "\n",
    "PEAK, VALLEY = 1, -1\n",
    "def _identify_initial_pivot(X, up_thresh, down_thresh):\n",
    "    x_0 = X[0]\n",
    "    max_x = x_0\n",
    "    max_t = 0\n",
    "    min_x = x_0\n",
    "    min_t = 0\n",
    "    up_thresh += 1\n",
    "    down_thresh += 1\n",
    "\n",
    "    for t in range(1, len(X)):\n",
    "        x_t = X[t]\n",
    "\n",
    "        if x_t / min_x >= up_thresh:\n",
    "            return VALLEY if min_t == 0 else PEAK\n",
    "\n",
    "        if x_t / max_x <= down_thresh:\n",
    "            return PEAK if max_t == 0 else VALLEY\n",
    "\n",
    "        if x_t > max_x:\n",
    "            max_x = x_t\n",
    "            max_t = t\n",
    "\n",
    "        if x_t < min_x:\n",
    "            min_x = x_t\n",
    "            min_t = t\n",
    "\n",
    "    t_n = len(X)-1\n",
    "    return VALLEY if x_0 < X[t_n] else PEAK\n",
    "\n",
    "def peak_valley_pivots_candlestick(close, high, low, up_thresh, down_thresh):\n",
    "    if down_thresh > 0:\n",
    "        raise ValueError('The down_thresh must be negative.')\n",
    "\n",
    "    initial_pivot = _identify_initial_pivot(close, up_thresh, down_thresh)\n",
    "\n",
    "    t_n = len(close)\n",
    "    pivots = np.zeros(t_n, dtype='i1')\n",
    "    pivots[0] = initial_pivot\n",
    "\n",
    "    up_thresh += 1\n",
    "    down_thresh += 1\n",
    "\n",
    "    trend = -initial_pivot\n",
    "    last_pivot_t = 0\n",
    "    last_pivot_x = close[0]\n",
    "    for t in range(1, len(close)):\n",
    "\n",
    "        if trend == -1:\n",
    "            x = low[t]\n",
    "            r = x / last_pivot_x\n",
    "            if r >= up_thresh:\n",
    "                pivots[last_pivot_t] = trend#\n",
    "                trend = 1\n",
    "                last_pivot_x = high[t]\n",
    "                last_pivot_t = t\n",
    "            elif x < last_pivot_x:\n",
    "                last_pivot_x = x\n",
    "                last_pivot_t = t\n",
    "        else:\n",
    "            x = high[t]\n",
    "            r = x / last_pivot_x\n",
    "            if r <= down_thresh:\n",
    "                pivots[last_pivot_t] = trend\n",
    "                trend = -1\n",
    "                last_pivot_x = low[t]\n",
    "                last_pivot_t = t\n",
    "            elif x > last_pivot_x:\n",
    "                last_pivot_x = x\n",
    "                last_pivot_t = t\n",
    "\n",
    "\n",
    "    if last_pivot_t == t_n-1:\n",
    "        pivots[last_pivot_t] = trend\n",
    "    elif pivots[t_n-1] == 0:\n",
    "        pivots[t_n-1] = trend\n",
    "\n",
    "    return pivots\n",
    "\n",
    "def my_pivothigh(series, left_bars, right_bars):\n",
    "    # pivot_high = None\n",
    "    pivot_range = left_bars + right_bars\n",
    "    # left_edge_value = series.shift(pivot_range)\n",
    "    \n",
    "    possible_pivot_high = series[right_bars:].shift(right_bars)\n",
    "    possible_pivot_high = np.concatenate([np.full(right_bars, np.nan), possible_pivot_high])\n",
    "\n",
    "    rolling_windows = np.lib.stride_tricks.sliding_window_view(series, window_shape=(pivot_range+1,))\n",
    "    max_previous_index = np.argmax(rolling_windows, axis=1)\n",
    "\n",
    "    max_previous_index = np.concatenate([np.full(pivot_range, np.nan), max_previous_index])\n",
    "    pivot_high_right_bars = pd.Series((pivot_range+1) - max_previous_index - 1)\n",
    "    condition = pivot_high_right_bars == left_bars\n",
    "    # result_series = pd.Series(np.where(condition, possible_pivot_high, 0))\n",
    "    result_series = pd.Series(np.where(condition, -1, 0))\n",
    "    result_series[result_series != -1] = 0\n",
    "    return result_series\n",
    "\n",
    "def my_pivotlow(series, left_bars, right_bars):\n",
    "    # pivot_low = None\n",
    "    pivot_range = left_bars + right_bars\n",
    "    # left_edge_value = series.shift(pivot_range)\n",
    "\n",
    "    possible_pivot_low = series[right_bars:].shift(right_bars)\n",
    "    possible_pivot_low = np.concatenate([np.full(right_bars, np.nan), possible_pivot_low])\n",
    "\n",
    "    rolling_windows = np.lib.stride_tricks.sliding_window_view(series, window_shape=(pivot_range+1,))\n",
    "    min_previous_index = np.argmin(rolling_windows, axis=1)\n",
    "\n",
    "    min_previous_index = np.concatenate([np.full(pivot_range, np.nan), min_previous_index])\n",
    "    pivot_low_right_bars = pd.Series((pivot_range+1) - min_previous_index - 1)\n",
    "    condition = pivot_low_right_bars == left_bars\n",
    "    # result_series = pd.Series(np.where(condition, possible_pivot_low, 0))\n",
    "    result_series = pd.Series(np.where(condition, 1, 0))\n",
    "    result_series[result_series != 1] = 0\n",
    "    return result_series\n",
    "\n",
    "def calculate_up_or_down(pivot_high,pivot_low,pivot_length):\n",
    "    high = pivot_high.copy()\n",
    "    low = pivot_low.copy()\n",
    "    high[high != -1] = 0\n",
    "    low[low != 1] = 0\n",
    "    result = high + low\n",
    "    result[result == 0] = np.nan\n",
    "    result = result.ffill()\n",
    "    result[len(result)-pivot_length:] = np.nan\n",
    "    return result\n",
    "\n",
    "\n",
    "def highestvalue(_src,_len):\n",
    "    rolling_windows = np.lib.stride_tricks.sliding_window_view(_src, window_shape=(_len,))\n",
    "    max_previous_value = np.max(rolling_windows, axis=1)\n",
    "    max_previous_value = np.concatenate([np.full(_len-1, np.nan), max_previous_value])\n",
    "    return max_previous_value\n",
    "\n",
    "def lowestvalue(_src,_len):\n",
    "    rolling_windows = np.lib.stride_tricks.sliding_window_view(_src, window_shape=(_len,))\n",
    "    max_previous_value = np.min(rolling_windows, axis=1)\n",
    "    max_previous_value = np.concatenate([np.full(_len-1, np.nan), max_previous_value])\n",
    "    return max_previous_value\n",
    "\n",
    "def change_occurred(arr):\n",
    "    differences = np.diff(arr)\n",
    "    differences = np.concatenate([np.full(1, 0), differences])\n",
    "    return differences != 0\n",
    "\n",
    "@jit()\n",
    "def set_h_l_value(high:np.ndarray,low:np.ndarray,direction:np.ndarray):\n",
    "    price_now_1 = low[0]\n",
    "    price_now_2 = high[0]\n",
    "    price_now = low[0]\n",
    "\n",
    "    price_index = 0\n",
    "    price_index_1 = 0\n",
    "    price_index_2 = 0\n",
    "\n",
    "    price_index_array_1 = np.zeros_like(high)\n",
    "    price_index_array_2 = np.zeros_like(high)\n",
    "    price_index_array = np.zeros_like(high)\n",
    "\n",
    "    for i in range(len(high)):\n",
    "        if direction[i] != direction[i-1]:\n",
    "            price_now_1 = price_now_2\n",
    "            price_index_1 = price_index_2\n",
    "            price_index_array_1[i] = price_index_1\n",
    "            price_now_2 = price_now\n",
    "            price_index_2 = price_index\n",
    "            price_index_array_2[i] = price_index_2\n",
    "\n",
    "        if direction[i] > 0:\n",
    "            if (high[i] > price_now_2):\n",
    "                price_now_2 = high[i]\n",
    "                price_index_2 = i\n",
    "                price_index_array_2[i] = price_index_2\n",
    "                price_now = low[i]\n",
    "                price_index = i\n",
    "                price_index_array[i] = price_index\n",
    "\n",
    "            if (low[i] < price_now):\n",
    "                price_now = low[i]\n",
    "                price_index = i\n",
    "                price_index_array[i] = price_index\n",
    "\n",
    "        if direction[i] < 0:\n",
    "            if (low[i] < price_now_2):\n",
    "                price_now_2 = low[i]\n",
    "                price_index_2 = i\n",
    "                price_index_array_2[i] = price_index_2\n",
    "                price_now = high[i]\n",
    "                price_index = i\n",
    "                price_index_array[i] = price_index\n",
    "\n",
    "            if (high[i] > price_now):\n",
    "                price_now = high[i]\n",
    "                price_index = i\n",
    "                price_index_array[i] = price_index\n",
    "    \n",
    "    return price_index_array_1,price_index_array_2\n",
    "\n",
    "@jit()\n",
    "def findBarSince(src:np.ndarray):\n",
    "    last_true_index = None\n",
    "    index_distance_array = np.zeros_like(src).astype(np.int64)\n",
    "    counter = 0\n",
    "    for i, _ in enumerate(src):\n",
    "        if src[i]:\n",
    "            last_true_index = i\n",
    "            counter = 0\n",
    "        elif last_true_index is not None:\n",
    "            if i > last_true_index:\n",
    "                counter = counter + 1\n",
    "                index_distance_array[i] = counter\n",
    "    return index_distance_array\n",
    "\n",
    "@jit()\n",
    "def find_ytrain(z1:np.ndarray,z2:np.ndarray,direction:np.ndarray,high:np.ndarray,low:np.ndarray):\n",
    "    cutted_z1 = z1[direction != np.roll(direction,1)]\n",
    "    cutted_z2 = z2[direction != np.roll(direction,1)]\n",
    "    cutted_direction = direction[direction != np.roll(direction,1)]\n",
    "\n",
    "    iterator = zip(cutted_z1,cutted_direction)\n",
    "    result = np.zeros_like(direction)\n",
    "    previous_index = 0\n",
    "    for i,value in enumerate(iterator):\n",
    "        result[previous_index:int(value[0])+1] = value[1]\n",
    "        \n",
    "        previous_index = int(value[0])+1\n",
    "\n",
    "    while previous_index < len(z1)-1:\n",
    "        data_index_remainder = previous_index\n",
    "        result_remainder = result[data_index_remainder:] \n",
    "        direction_remainder = direction[data_index_remainder:]\n",
    "        high_remainder = high[data_index_remainder:]\n",
    "        low_remainder = low[data_index_remainder:]\n",
    "\n",
    "        max_index = np.argmax(high_remainder)+1\n",
    "        min_index = np.argmin(low_remainder)+1\n",
    "\n",
    "        if(result[data_index_remainder-1] == 1):\n",
    "            result_remainder[:max_index] = -1\n",
    "            result_remainder[max_index:] = 0\n",
    "            result_remainder = np.roll(result_remainder,1)\n",
    "            result_remainder[0] = -1\n",
    "            result[data_index_remainder:]  = result_remainder\n",
    "            previous_index = data_index_remainder + max_index\n",
    "        if(result[data_index_remainder-1] == -1):\n",
    "            result_remainder[:min_index] = 1   \n",
    "            result_remainder[min_index:] = 0\n",
    "            result_remainder = np.roll(result_remainder,1)\n",
    "            result_remainder[0] = 1\n",
    "            result[data_index_remainder:]  = result_remainder\n",
    "            previous_index = data_index_remainder + min_index\n",
    "        \n",
    "\n",
    "    # 1 into -1 and -1 into 1 Conversion\n",
    "    mask_1 = result == 1\n",
    "    mask_minus1 = result == -1\n",
    "    result[mask_1] = -1\n",
    "    result[mask_minus1] = 1\n",
    "    return result\n",
    "\n",
    "def zigzagpp(_high,_low,depth,deviation,backstep):\n",
    "    df = pd.DataFrame()\n",
    "    df['high'] = pd.Series(_high)\n",
    "    df['low'] = pd.Series(_low)\n",
    "\n",
    "    df['highest'] = highestvalue(df['high'],depth)\n",
    "    hr_condition = np.logical_not(((df['highest'] - df['high']) > (deviation * tick_size)).shift(1))\n",
    "    hr_condition = np.array(hr_condition, dtype=np.float64)\n",
    "    df['hr'] = findBarSince(hr_condition)\n",
    "\n",
    "    df['lowest'] = lowestvalue(df['low'],depth)\n",
    "    lr_condition = np.logical_not(((df['low'] - df['lowest']) > (deviation * tick_size)).shift(1))\n",
    "    lr_condition = np.array(lr_condition, dtype=np.float64)\n",
    "    df['lr'] = findBarSince(lr_condition)\n",
    "\n",
    "    difference_of_hr_lr_condition = np.logical_not(df['hr'] > df['lr'])\n",
    "    difference_of_hr_lr_condition = np.array(difference_of_hr_lr_condition,dtype=np.bool_)\n",
    "    difference_of_hr_lr = findBarSince(difference_of_hr_lr_condition) >= backstep\n",
    "    df['direction'] = np.where(difference_of_hr_lr,-1,1)\n",
    "\n",
    "    price_index_1 , price_index_2 = set_h_l_value(df['high'].values,df['low'].values,df['direction'].values)\n",
    "\n",
    "    price_index_1 = pd.Series(price_index_1)\n",
    "    price_index_2 = pd.Series(price_index_2)\n",
    "\n",
    "    return price_index_1,price_index_2,df['direction']\n",
    "\n",
    "def adaptiveTrendFinder(close: np.ndarray,periods:np.ndarray):\n",
    "    logSource = np.log(close)\n",
    "    trend_direction = np.zeros_like(close,dtype=np.float64)\n",
    "    detectedPeriod = np.zeros_like(close,dtype=np.float64)\n",
    "    highestPearsonR = np.zeros_like(close,dtype=np.float64)\n",
    "    for idx, current_value in enumerate(close):\n",
    "        result = calculate_trend_direction(idx, close ,logSource, periods)\n",
    "        trend_direction[idx] = result[0]\n",
    "        detectedPeriod[idx] = result[1]\n",
    "        highestPearsonR[idx] = result[2]\n",
    "    return trend_direction,detectedPeriod,highestPearsonR\n",
    "\n",
    "def calculate_trend_direction(x:int, close:np.ndarray,logSource:np.ndarray, periods:np.ndarray):\n",
    "    # if(x == len(dataframe)-1) | (x.name == len(dataframe)-2):\n",
    "    if (x >= periods[2]):\n",
    "        devMultiplier = 2.0\n",
    "        # Calculate Deviation,PersionR,Slope,Intercept\n",
    "        stdDev01, pearsonR01, slope01, intercept01 = calcDev(\n",
    "            periods[1], close, logSource , x)\n",
    "        stdDev02, pearsonR02, slope02, intercept02 = calcDev(\n",
    "            periods[2], close, logSource , x)\n",
    "        stdDev03, pearsonR03, slope03, intercept03 = calcDev(\n",
    "            periods[3], close, logSource , x)\n",
    "        stdDev04, pearsonR04, slope04, intercept04 = calcDev(\n",
    "            periods[4], close, logSource , x)\n",
    "        # Find the highest Pearson's R\n",
    "        highestPearsonR = max(pearsonR01, pearsonR02,pearsonR03,pearsonR04)\n",
    "\n",
    "        # Determine selected length, slope, intercept, and deviations\n",
    "        detectedPeriod = 0\n",
    "        detectedSlope = 0\n",
    "        detectedIntrcpt = 0\n",
    "        detectedStdDev = 0\n",
    "\n",
    "        if highestPearsonR == pearsonR01:\n",
    "            detectedPeriod = periods[1]\n",
    "            detectedSlope = slope01\n",
    "            detectedIntrcpt = intercept01\n",
    "            detectedStdDev = stdDev01\n",
    "        elif highestPearsonR == pearsonR02:\n",
    "            detectedPeriod = periods[2]\n",
    "            detectedSlope = slope02\n",
    "            detectedIntrcpt = intercept02\n",
    "            detectedStdDev = stdDev02\n",
    "        elif highestPearsonR == pearsonR03:\n",
    "            detectedPeriod = periods[3]\n",
    "            detectedSlope = slope03\n",
    "            detectedIntrcpt = intercept03\n",
    "            detectedStdDev = stdDev03\n",
    "        elif highestPearsonR == pearsonR04:\n",
    "            detectedPeriod = periods[4]\n",
    "            detectedSlope = slope04\n",
    "            detectedIntrcpt = intercept04\n",
    "            detectedStdDev = stdDev04\n",
    "        else:\n",
    "            # Default case\n",
    "            raise Exception(f\"Cannot Find Highest PearsonR\")\n",
    "\n",
    "        # Calculate start and end price based on detected slope and intercept\n",
    "        startPrice = math.exp(\n",
    "            detectedIntrcpt + detectedSlope * (detectedPeriod - 1))\n",
    "        endPrice = math.exp(detectedIntrcpt)\n",
    "\n",
    "        trend_direction = endPrice - startPrice\n",
    "        return (trend_direction, detectedPeriod, highestPearsonR)\n",
    "    return (0, 0, 0)\n",
    "\n",
    "@jit()\n",
    "def calcDev(length: int, close: np.ndarray, logSource: np.ndarray, index: int):\n",
    "    period_1 = length - 1\n",
    "    sumX = 0.0\n",
    "    sumXX = 0.0\n",
    "    sumYX = 0.0\n",
    "    sumY = 0.0\n",
    "    for i in range(1, length+1):\n",
    "        lSrc = logSource[index+1-i]\n",
    "        sumX += i\n",
    "        sumXX += i * i\n",
    "        sumYX += i * lSrc\n",
    "        sumY += lSrc\n",
    "\n",
    "    slope = np.nan_to_num((length * sumYX - sumX * sumY) /\n",
    "                          (length * sumXX - sumX * sumX))\n",
    "    average = sumY / length\n",
    "    intercept = average - (slope * sumX / length) + slope\n",
    "    sumDev = 0.0\n",
    "    sumDxx = 0.0\n",
    "    sumDyy = 0.0\n",
    "    sumDyx = 0.0\n",
    "    regres = intercept + slope * period_1 * 0.5\n",
    "    sumSlp = intercept\n",
    "\n",
    "    for i in range(1, period_1+1):\n",
    "        lSrc = logSource[index+1-i]\n",
    "        dxt = lSrc - average\n",
    "        dyt = sumSlp - regres\n",
    "        lSrc -= sumSlp\n",
    "        sumSlp += slope\n",
    "        sumDxx += dxt * dxt\n",
    "        sumDyy += dyt * dyt\n",
    "        sumDyx += dxt * dyt\n",
    "        sumDev += lSrc * lSrc\n",
    "\n",
    "    unStdDev = math.sqrt(sumDev / period_1)\n",
    "    divisor = sumDxx * sumDyy\n",
    "    if divisor == 0 or np.isnan(divisor):\n",
    "        pearsonR = 0  # Set Pearson correlation coefficient to NaN\n",
    "    else:\n",
    "        pearsonR = sumDyx / math.sqrt(divisor)\n",
    "\n",
    "    return unStdDev, pearsonR, slope, intercept\n",
    "\n",
    "\n",
    "def calculateSTCIndicator(dataframe, length, fastLength, slowLength):\n",
    "    df = dataframe.copy()\n",
    "    EEEEEE = length\n",
    "    BBBB = fastLength\n",
    "    BBBBB = slowLength\n",
    "    # dataframe = heikinashi(dataframe)\n",
    "    mAAAAA = AAAAA(df, EEEEEE, BBBB, BBBBB)\n",
    "    return mAAAAA\n",
    "\n",
    "\n",
    "def AAAA(BBB, BBBB, BBBBB):\n",
    "    fastMA = ta.EMA(BBB, timeperiod=BBBB)\n",
    "    slowMA = ta.EMA(BBB, timeperiod=BBBBB)\n",
    "    AAAA = fastMA - slowMA\n",
    "    return AAAA\n",
    "\n",
    "@jit()\n",
    "def formula_of_AAAAA(output:np.ndarray,input:np.ndarray,length:int,AAA:int):\n",
    "    for i in range(0, length):\n",
    "        if (i > 0):\n",
    "            output[i] = output[i-1] + (AAA * (input[i] - output[i-1]))\n",
    "\n",
    "    return output\n",
    "\n",
    "def AAAAA(dataframe, EEEEEE, BBBB, BBBBB):\n",
    "\n",
    "    AAA = 0.5\n",
    "    dataframe['DDD'] = 0.0\n",
    "    dataframe['CCCCC'] = 0\n",
    "    dataframe['DDDDDD'] = 0\n",
    "    dataframe['EEEEE'] = 0.0\n",
    "\n",
    "    dataframe['BBBBBB'] = AAAA(dataframe['close'], BBBB, BBBBB)\n",
    "    dataframe['CCC'] = dataframe['BBBBBB'].rolling(window=EEEEEE).min()\n",
    "    dataframe['CCCC'] = dataframe['BBBBBB'].rolling(\n",
    "        window=EEEEEE).max() - dataframe['CCC']\n",
    "    dataframe['CCCCC'] = np.where(dataframe['CCCC'] > 0, (dataframe['BBBBBB'] -\n",
    "                                  dataframe['CCC']) / dataframe['CCCC'] * 100, dataframe['CCCCC'].shift(1).fillna(0))\n",
    "    \n",
    "    dataframe['DDD'] = formula_of_AAAAA(dataframe['DDD'].values,dataframe['CCCCC'].values,len(dataframe),AAA)\n",
    "\n",
    "    dataframe['DDDD'] = dataframe['DDD'].rolling(window=EEEEEE).min()\n",
    "    dataframe['DDDDD'] = dataframe['DDD'].rolling(\n",
    "        window=EEEEEE).max() - dataframe['DDDD']\n",
    "    dataframe['DDDDDD'] = np.where(dataframe['DDD'] > 0, (dataframe['DDD'] - dataframe['DDDD']) /\n",
    "                                   dataframe['DDDDD'] * 100, dataframe['DDD'].fillna(dataframe['DDDDDD'].shift(1)))\n",
    "\n",
    "    dataframe['EEEEE'] = formula_of_AAAAA(dataframe['EEEEE'].values,dataframe['DDDDDD'].values,len(dataframe),AAA)\n",
    "\n",
    "    return dataframe['EEEEE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heikinashi(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_HA = df.copy()\n",
    "    df_HA['close'] = (df_HA['open'] + df_HA['high'] +\n",
    "                      df_HA['low'] + df_HA['close']) / 4\n",
    "\n",
    "    for i in range(0, len(df_HA)):\n",
    "        if i == 0:\n",
    "            df_HA.loc[i, 'open'] = (\n",
    "                (df_HA.loc[i, 'open'] + df_HA.loc[i, 'close']) / 2)\n",
    "        else:\n",
    "            df_HA.loc[i, 'open'] = (\n",
    "                (df_HA.loc[i-1, 'open'] + df_HA.loc[i-1, 'close']) / 2)\n",
    "\n",
    "    df_HA['high'] = df_HA[['open', 'close', 'high']].max(axis=1)\n",
    "    df_HA['low'] = df_HA[['open', 'close', 'low']].min(axis=1)\n",
    "\n",
    "    return df_HA\n",
    "\n",
    "\n",
    "class FeatureName(Enum):\n",
    "    rsi = \"RSI\"\n",
    "    wt = \"WT\"\n",
    "    cci = \"CCI\"\n",
    "    adx = \"ADX\"\n",
    "    ema = \"EMA\"\n",
    "    sma = \"SMA\"\n",
    "    macd = \"MACD\"\n",
    "    tema = \"TEMA\"\n",
    "    open = \"OPEN\"\n",
    "    high = \"HIGH\"\n",
    "    low = \"LOW\"\n",
    "    close = \"CLOSE\"\n",
    "    volume = \"VOLUME\"\n",
    "    all = \"ALL\"\n",
    "    trend = \"TREND\"\n",
    "    stc = \"STC\"\n",
    "\n",
    "def chooseFeatureName(name: FeatureName, dataframe, paramsA, paramsB):\n",
    "    df = dataframe.copy()\n",
    "    source = df['close']\n",
    "    hlc3 = (df['high'] + df['low'] + df['close']) / 3\n",
    "\n",
    "    if (name.name == FeatureName.open.name):\n",
    "        return df['open']\n",
    "    if (name.name == FeatureName.high.name):\n",
    "        return df['high']\n",
    "    if (name.name == FeatureName.low.name):\n",
    "        return df['low']\n",
    "    if (name.name == FeatureName.close.name):\n",
    "        return df['close']\n",
    "    if (name.name == FeatureName.volume.name):\n",
    "        return df['volume']\n",
    "    if (name.name == FeatureName.rsi.name):\n",
    "        return n_rsi(source, paramsA, paramsB)\n",
    "    if (name.name == FeatureName.wt.name):\n",
    "        return n_wt(hlc3, paramsA, paramsB)\n",
    "    if (name.name == FeatureName.cci.name):\n",
    "        return n_cci(df, paramsA, paramsB)\n",
    "    if (name.name == FeatureName.adx.name):\n",
    "        return n_adx_optimized(df['high'], df['low'], source,paramsA)\n",
    "    if (name.name == FeatureName.ema.name):\n",
    "        ema = ta.EMA(df,paramsA) - ta.EMA(df,paramsB)\n",
    "        old_min = ema.min()\n",
    "        old_max = ema.max()\n",
    "        return rescale(ema,old_min,old_max,0,1)\n",
    "    if (name.name == FeatureName.sma.name):\n",
    "        sma = ta.SMA(df,paramsA) - ta.SMA(df,paramsB)\n",
    "        old_min = sma.min()\n",
    "        old_max = sma.max()\n",
    "        return rescale(sma,old_min,old_max,0,1)\n",
    "    if (name.name == FeatureName.macd.name):\n",
    "        macd = ta.MACD(df)['macdhist']\n",
    "        old_min = macd.min()\n",
    "        old_max = macd.max()\n",
    "        return rescale(macd,old_min,old_max,0,1) \n",
    "    if (name.name == FeatureName.tema.name):\n",
    "        tema = ta.TEMA(df,paramsA) - ta.TEMA(df,paramsB)\n",
    "        old_min = tema.min()\n",
    "        old_max = tema.max()\n",
    "        return rescale(tema,old_min,old_max,0,1)\n",
    "    if (name.name == FeatureName.all.name):\n",
    "        all_value = n_rsi(source, paramsA, paramsB) + n_wt(hlc3, paramsA, paramsB) + n_cci(df, paramsA, paramsB) +n_adx_optimized(df['high'], df['low'], source,paramsA)\n",
    "        old_min = all_value.min()\n",
    "        old_max = all_value.max()\n",
    "        return rescale(all_value,old_min,old_max,0,1)\n",
    "    if (name.name == FeatureName.trend.name):\n",
    "        df['trend_direction'],df['detectedPeriod'],df['highestPearsonR'] = adaptiveTrendFinder(source.values,paramsA)\n",
    "        return df['trend_direction']+df['highestPearsonR']\n",
    "    if (name.name == FeatureName.stc.name):\n",
    "        fast_length = int(paramsA.split(\"-\")[0])\n",
    "        slow_length = int(paramsA.split(\"-\")[1])\n",
    "        length = paramsB\n",
    "        return calculateSTCIndicator(dataframe,length,fast_length,slow_length)\n",
    "\n",
    "def compare_value_improved(dataframe:pd.DataFrame,future_count:int):\n",
    "    df = dataframe.copy()\n",
    "    # BTC\n",
    "    # pivots = peak_valley_pivots_candlestick(df['close'], df['high'], df['low'] ,.008,-.008)\n",
    "    # 1000BONK\n",
    "    pivots = peak_valley_pivots_candlestick(df['close'], df['high'], df['low'] ,.02,-.02)\n",
    "\n",
    "    df['Pivots'] = pivots\n",
    "    df['y_train'] = df['Pivots'].replace(0, np.nan)\n",
    "    df['y_train'] = df['y_train'].ffill()\n",
    "    df['y_train'] = df['y_train'].fillna(0)\n",
    "    return df['y_train']\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(dataframe: pd.DataFrame, training_params):\n",
    "    df = dataframe.copy()\n",
    "    future_count = training_params['future_count']\n",
    "    feature_count = training_params['feature_count']\n",
    "    pivot_depth = training_params['pivot_params']['depth']\n",
    "    pivot_deviation = training_params['pivot_params']['deviation']\n",
    "    pivot_backstep = training_params['pivot_params']['backstep']\n",
    "\n",
    "    # f1_name = training_params['f1']['name']\n",
    "    # f1_param_A = training_params['f1']['paramsA']\n",
    "    # f1_param_B = training_params['f1']['paramsB']\n",
    "\n",
    "    # f2_name = training_params['f2']['name']\n",
    "    # f2_param_A = training_params['f2']['paramsA']\n",
    "    # f2_param_B = training_params['f2']['paramsB']\n",
    "\n",
    "    # f3_name = training_params['f3']['name']\n",
    "    # f3_param_A = training_params['f3']['paramsA']\n",
    "    # f3_param_B = training_params['f3']['paramsB']\n",
    "\n",
    "    # f4_name = training_params['f4']['name']\n",
    "    # f4_param_A = training_params['f4']['paramsA']\n",
    "    # f4_param_B = training_params['f4']['paramsB']\n",
    "\n",
    "    # f5_name = training_params['f5']['name']\n",
    "    # f5_param_A = training_params['f5']['paramsA']\n",
    "    # f5_param_B = training_params['f5']['paramsB']\n",
    "    for i in range(1,feature_count+1):\n",
    "        df[f'f{i}'] = chooseFeatureName(training_params[f'f{i}']['name'], df, training_params[f'f{i}']['paramsA'], training_params[f'f{i}']['paramsB'])\n",
    "\n",
    "    # df['pivots_high'] = pd.Series(my_pivothigh(df['high'],pivot_params,pivot_params)).shift(-pivot_params)\n",
    "    # df['pivots_low'] = pd.Series(my_pivotlow(df['low'],pivot_params,pivot_params)).shift(-pivot_params)\n",
    "    # df['y_train'] = calculate_up_or_down(df['pivots_high'],df['pivots_low'],pivot_params)\n",
    "    # df['y_train'] = compare_value_improved(df,future_count)\n",
    "    # df['y_train'] = (df_shifted['close'] < dataframe['close']).astype(int)\n",
    "    # df['y_train'] = df['y_train'].shift(-future_count)\n",
    "    # df.loc[len(df)-(future_count):,['y_train']] = np.nan\n",
    "\n",
    "    df['z1'],df['z2'],df['direction'] = zigzagpp(df['high'],df['low'],pivot_depth,pivot_deviation,pivot_backstep)\n",
    "    if(future_count > 0):\n",
    "        df['y_train'] = pd.Series(find_ytrain(df['z1'].values,df['z2'].values,df['direction'].values,df['high'].values,df['low'].values)).shift(-future_count)\n",
    "    else:\n",
    "        df['y_train'] = pd.Series(find_ytrain(df['z1'].values,df['z2'].values,df['direction'].values,df['high'].values,df['low'].values))\n",
    "    df.loc[df['y_train'] == 0,['y_train']] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_volatility(dataframe:pd.DataFrame,minLength:int,maxLength:int):\n",
    "    df = dataframe.copy()\n",
    "    recentAtr = ta.ATR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=minLength)\n",
    "    historicalAtr = ta.ATR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=maxLength)\n",
    "    return recentAtr > historicalAtr\n",
    "\n",
    "def regime_filter(dataframe, threshold):\n",
    "    df = dataframe.copy()\n",
    "    ohlc4 = (df['open'] + df['high'] + df['low'] + df['close']) / 4\n",
    "    src = ohlc4\n",
    "\n",
    "    value1 = pd.Series(0,index=df['close'].index, dtype=float)\n",
    "    value2 = pd.Series(0,index=df['close'].index, dtype=float)\n",
    "    klmf = pd.Series(0,index=df['close'].index, dtype=float)\n",
    "\n",
    "    for i in range(0, len(value1)):\n",
    "        if(i == 0):\n",
    "            value1[i] = 0\n",
    "        else:\n",
    "            value1[i] = 0.2 * (src[i] - src[i-1]) + 0.8 * value1[i-1]\n",
    "\n",
    "    for i in range(0, len(value1)):\n",
    "        if(i == 0):\n",
    "            value2[i] = 0.1 * (df['high'][i] - df['low'][i]) + 0.8 * 0\n",
    "        else:\n",
    "            value2[i] = 0.1 * (df['high'][i] - df['low'][i]) + 0.8 * value2[i-1]\n",
    "\n",
    "    omega = abs(value1 / value2)\n",
    "    alpha = (-omega**2 + np.sqrt(omega**4 + 16 * omega**2)) / 8 \n",
    "\n",
    "    for i in range(0, len(value1)):\n",
    "        if(i == 0):\n",
    "            klmf[i] = alpha[i] * src[i] + (1 - alpha[i]) * 0\n",
    "        else:\n",
    "            klmf[i] = alpha[i] * src[i] + (1 - alpha[i]) * klmf[i-1]\n",
    "\n",
    "    absCurveSlope = klmf.diff().abs()\n",
    "    exponentialAverageAbsCurveSlope = 1.0 * ta.EMA(absCurveSlope, 200)\n",
    "    normalized_slope_decline = (absCurveSlope - exponentialAverageAbsCurveSlope) / exponentialAverageAbsCurveSlope\n",
    "    return normalized_slope_decline >= threshold\n",
    "\n",
    "def ema_filter(dataframe,period):\n",
    "    df = dataframe.copy()\n",
    "    ema = ta.EMA(df['close'], period)\n",
    "    filter_value = (df['close'] > ema).astype(int) - (df['close'] < ema).astype(int)\n",
    "    return filter_value\n",
    "\n",
    "def sma_filter(dataframe,period):\n",
    "    df = dataframe.copy()\n",
    "    sma = ta.SMA(df['close'], period)\n",
    "    filter_value = (df['close'] > sma).astype(int) - (df['close'] < sma).astype(int)\n",
    "    return filter_value\n",
    "\n",
    "def kernel_filter(dataframe,loopback,relative_weight,start_at_bar):\n",
    "    df = dataframe.copy()\n",
    "    khat1 = pd.Series(rational_quadratic(df['close'], loopback, relative_weight, start_at_bar))\n",
    "    # wasBearishRate = khat1.shift(2) > khat1.shift(1)\n",
    "    # isBearishRate = khat1.shift(1) > khat1\n",
    "    # wasBullishRate = khat1.shift(2) < khat1.shift(1)\n",
    "    filter_rate = (khat1.shift(1) < khat1).astype(int) - (khat1.shift(1) > khat1).astype(int)\n",
    "    return filter_rate\n",
    "\n",
    "def rational_quadratic(\n",
    "    price_feed: np.ndarray,\n",
    "    lookback: int,\n",
    "    relative_weight: float,\n",
    "    start_at_bar: int,\n",
    ") -> np.ndarray:\n",
    "    length_of_prices = len(price_feed)\n",
    "    bars_calculated = start_at_bar + 1\n",
    "\n",
    "    result = np.zeros(length_of_prices, dtype=float)\n",
    "    lookback_squared = np.power(lookback, 2)\n",
    "    denominator = lookback_squared * 2 * relative_weight\n",
    "\n",
    "    for index in range(length_of_prices):\n",
    "        current_weight = 0.0\n",
    "        cumulative_weight = 0.0\n",
    "\n",
    "        for i in range(bars_calculated):\n",
    "            y = np.nan if (index - i) < 0 else price_feed[index - i]\n",
    "            w = np.power(\n",
    "                1 + (np.power(i, 2) / denominator),\n",
    "                -relative_weight,\n",
    "            )\n",
    "            current_weight += y * w\n",
    "            cumulative_weight += w\n",
    "\n",
    "        result[index] = current_weight / cumulative_weight\n",
    "\n",
    "    return result\n",
    "\n",
    "def gaussian(\n",
    "    price_feed: np.ndarray,\n",
    "    lookback: int,\n",
    "    start_at_bar: int,\n",
    ") -> np.ndarray:\n",
    "    length_of_prices = len(price_feed)\n",
    "    bars_calculated = start_at_bar + 1\n",
    "\n",
    "    result = np.zeros(length_of_prices, dtype=float)\n",
    "    lookback_squared = np.power(lookback, 2)\n",
    "    denominator = lookback_squared * 2\n",
    "\n",
    "    for index in range(length_of_prices):\n",
    "        current_weight = 0.0\n",
    "        cumulative_weight = 0.0\n",
    "\n",
    "        for i in range(bars_calculated):\n",
    "            y = np.nan if (index - i) < 0 else price_feed[index - i]\n",
    "            w = np.exp(-(np.power(i, 2) / denominator))\n",
    "            current_weight += y * w\n",
    "            cumulative_weight += w\n",
    "\n",
    "        result[index] = current_weight / cumulative_weight\n",
    "\n",
    "    return result\n",
    "\n",
    "def xATRTrailingStop_func(close, prev_close, prev_atr, nloss):\n",
    "    if close > prev_atr and prev_close > prev_atr:\n",
    "        return max(prev_atr, close - nloss)\n",
    "    elif close < prev_atr and prev_close < prev_atr:\n",
    "        return min(prev_atr, close + nloss)\n",
    "    elif close > prev_atr:\n",
    "        return close - nloss\n",
    "    else:\n",
    "        return close + nloss\n",
    "\n",
    "def calculate_ut_bot(dataframe, SENSITIVITY, ATR_PERIOD):\n",
    "    df = dataframe.copy()\n",
    "    # UT Bot Parameters\n",
    "    SENSITIVITY = SENSITIVITY\n",
    "    ATR_PERIOD = ATR_PERIOD\n",
    "\n",
    "    # Compute ATR And nLoss variable\n",
    "    df[\"xATR\"] = ta.ATR(\n",
    "        df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=ATR_PERIOD)\n",
    "    df[\"nLoss\"] = SENSITIVITY * df[\"xATR\"]\n",
    "\n",
    "    df[\"ATRTrailingStop\"] = [0.0] + \\\n",
    "        [np.nan for i in range(len(df) - 1)]\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        df.loc[i, \"ATRTrailingStop\"] = xATRTrailingStop_func(\n",
    "            df.loc[i, \"close\"],\n",
    "            df.loc[i - 1, \"close\"],\n",
    "            df.loc[i - 1, \"ATRTrailingStop\"],\n",
    "            df.loc[i, \"nLoss\"],\n",
    "        )\n",
    "\n",
    "    # Buy Signal\n",
    "    df.loc[\n",
    "        (\n",
    "            (df[\"close\"] > df[\"ATRTrailingStop\"])\n",
    "        ),\n",
    "        'UT_Signal'] = 1\n",
    "\n",
    "    df.loc[\n",
    "        (\n",
    "            (df[\"close\"] < df[\"ATRTrailingStop\"])\n",
    "        ),\n",
    "        'UT_Signal'] = -1\n",
    "\n",
    "    return df['UT_Signal']\n",
    "\n",
    "\n",
    "class Filter(Enum):\n",
    "    volatility = \"filter_volatility\"\n",
    "    regime = \"regime_filter\"\n",
    "    trend = \"trend_filter\"\n",
    "    stc = \"stc_filter\"\n",
    "    ut = \"ut_filter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLorentizanDistance(i,current_feature,feature_array):\n",
    "    feature_distance = math.log(1+ abs(current_feature - feature_array[i]))\n",
    "    return feature_distance\n",
    "\n",
    "def fractalFilters(predict_value:pd.Series):\n",
    "    isDifferentSignalType = predict_value.ne(predict_value.shift())\n",
    "    return isDifferentSignalType\n",
    "\n",
    "def setPredictionAsClearWay(index,dataframe:pd.DataFrame,filter_method):\n",
    "    df = dataframe.copy()\n",
    "    global signal_predictions\n",
    "    prediction_value = 0\n",
    "    predicted_value = df['predicted_value'].iloc[index]\n",
    "    filter_value = True\n",
    "\n",
    "    buy_trend = True\n",
    "    sell_trend = True\n",
    "\n",
    "    buy_stc = True\n",
    "    sell_stc = True\n",
    "\n",
    "    buy_ut = True\n",
    "    sell_ut = True\n",
    "\n",
    "    for filter in filter_method:\n",
    "        if(filter.name == \"trend\"):\n",
    "            buy_trend = (df[filter.value].iloc[index] > 0)\n",
    "            sell_trend = (df[filter.value].iloc[index] < 0)\n",
    "        elif(filter.name == \"stc\"):\n",
    "            buy_stc = (df[filter.value].iloc[index] > 0)\n",
    "            sell_stc = (df[filter.value].iloc[index] < 0)\n",
    "        elif(filter.name == \"ut\"):\n",
    "            buy_ut = (df[filter.value].iloc[index] > 0)\n",
    "            sell_ut = (df[filter.value].iloc[index] > 0)\n",
    "        else:\n",
    "            filter_value = (df[filter.value].iloc[index]) and (filter_value)\n",
    "\n",
    "    if (predicted_value > 0) & filter_value & buy_trend & buy_stc & buy_ut:\n",
    "        prediction_value = 1\n",
    "    elif (predicted_value < 0) & filter_value & sell_trend & sell_stc & sell_ut:\n",
    "        prediction_value = -1\n",
    "    else:\n",
    "        if index == 0:\n",
    "            prediction_value = 0\n",
    "        else:\n",
    "            prediction_value = signal_predictions[index-1]\n",
    "    signal_predictions[index] = prediction_value\n",
    "    return prediction_value\n",
    "\n",
    "def split_predictions_with_proba(current_predict_class,current_predict_probability,dataframe,n_predict_candle) -> dict:\n",
    "    final_predictions = {}\n",
    "    df_for_predict = dataframe.copy()\n",
    "\n",
    "    current_predictions_with_prob = zip(\n",
    "        current_predict_class, current_predict_probability)\n",
    "\n",
    "    for i, (pred_class, prob) in enumerate(current_predictions_with_prob, 1):\n",
    "        predict_minute = (\n",
    "            df_for_predict.iloc[i-1]['date'].minute + n_predict_candle) % 60\n",
    "        predict_class = pred_class\n",
    "        predict_probability = prob\n",
    "\n",
    "        final_predictions[predict_minute] = {\n",
    "            \"index\": i,\n",
    "            \"class\": predict_class,\n",
    "            \"probability\": predict_probability\n",
    "        }\n",
    "    return final_predictions\n",
    "\n",
    "# def train_model(index,df,training_params):\n",
    "#     current_index = index\n",
    "#     lastDistance = -1.0\n",
    "#     neighbour_count = training_params['neighbor_count']\n",
    "#     feature_count = training_params['feature_count']\n",
    "#     # Variable Used for ML\n",
    "#     global distances\n",
    "#     global predictions\n",
    "\n",
    "#     feature_array_1 = df['f1'].to_numpy()\n",
    "#     feature_array_2 = df['f2'].to_numpy()\n",
    "#     feature_array_3 = df['f3'].to_numpy()\n",
    "#     feature_array_4 = df['f4'].to_numpy()\n",
    "#     feature_array_5 = df['f5'].to_numpy()\n",
    "#     y_train_array = df['y_train'].to_numpy()\n",
    "\n",
    "#     current_feature_1 =  feature_array_1[current_index]\n",
    "#     current_feature_2 =  feature_array_2[current_index]\n",
    "#     current_feature_3 =  feature_array_3[current_index]\n",
    "#     current_feature_4 =  feature_array_4[current_index]\n",
    "#     current_feature_5 =  feature_array_5[current_index] \n",
    "\n",
    "#     feature_array_1 = feature_array_1[:current_index+1]\n",
    "#     feature_array_2 = feature_array_2[:current_index+1]\n",
    "#     feature_array_3 = feature_array_3[:current_index+1]\n",
    "#     feature_array_4 = feature_array_4[:current_index+1]\n",
    "#     feature_array_5 = feature_array_5[:current_index+1]\n",
    "\n",
    "#     y_train_array = y_train_array[:current_index+1]\n",
    "\n",
    "#     for i in range(0,current_index+1,1):\n",
    "#         d = 0\n",
    "#         current_feature_names = [current_feature_1, current_feature_2,current_feature_3,current_feature_4,current_feature_5]\n",
    "#         feature_array_names = [feature_array_1, feature_array_2,feature_array_3,feature_array_4,feature_array_5]\n",
    "#         current_feature_names = current_feature_names[:feature_count]\n",
    "#         feature_array_names = feature_array_names[:feature_count]\n",
    "\n",
    "#         for var_index,_ in enumerate(current_feature_names):\n",
    "#             current_feature_count = current_feature_names[var_index]\n",
    "#             feature_array_count = feature_array_names[var_index]\n",
    "#             d = getLorentizanDistance(i,current_feature_count,feature_array_count) + d\n",
    "        \n",
    "#         if (d >= lastDistance) and (i%4):\n",
    "#             lastDistance = d\n",
    "#             distances.append(d)\n",
    "#             predictions.append(round(y_train_array[i]))\n",
    "#             if len(predictions) > neighbour_count:\n",
    "#                 lastDistance = distances[round(neighbour_count*3/4)]\n",
    "#                 distances.pop(0)\n",
    "#                 predictions.pop(0)\n",
    "    \n",
    "#     prediction = sum(predictions)\n",
    "#     return prediction\n",
    "\n",
    "def train_model(dataframe,training_params):\n",
    "    # Variable To Use\n",
    "    feature_count = training_params[\"feature_count\"]\n",
    "    future_count = training_params[\"future_count\"]\n",
    "\n",
    "    # Copy Dataframe\n",
    "    df = dataframe.copy()\n",
    "    df_for_predict = dataframe.copy()\n",
    "\n",
    "    # Dropping NAN Value\n",
    "    df.dropna(inplace=True)\n",
    "    df_for_predict.fillna(0,inplace=True)\n",
    "\n",
    "    # Extract Feature List In data_x and data_y for training and predicting\n",
    "    feature_list = [f'f{i}' for i in range(1, feature_count + 1)]\n",
    "    data_x = df[feature_list].values\n",
    "    data_y = df['y_train'].values\n",
    "    data_for_predict = df_for_predict[feature_list].values\n",
    "\n",
    "    # Scaling Using StandardScaler\n",
    "    scaler_for_train = StandardScaler()\n",
    "    data_x = scaler_for_train.fit_transform(data_x)\n",
    "\n",
    "    scaler_for_test = StandardScaler()\n",
    "    data_for_predict = scaler_for_test.fit_transform(data_for_predict)\n",
    "\n",
    "    # Split Data For Training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=60/num_candles_to_fetch, random_state=42)\n",
    "    # Build Model\n",
    "    xgb_model = XGBClassifier(\n",
    "        tree_method=\"auto\",\n",
    "    )\n",
    "    model = xgb_model\n",
    "    a = datetime.now()\n",
    "    model.fit(X_train, y_train)\n",
    "    joblib.dump(model, '../../trained_model.pkl')\n",
    "    b = datetime.now()\n",
    "    # Predict Model\n",
    "    length_to_predict = data_for_predict.shape[0]-future_count\n",
    "    current_predict_class = model.predict(data_for_predict[:length_to_predict])\n",
    "    current_predict_class[current_predict_class == 0] = -1\n",
    "    current_predict_probability = model.predict_proba(data_for_predict[:length_to_predict])\n",
    "    for _ in range(length_to_predict,length_to_predict+future_count):\n",
    "        current_predict_class = np.append(current_predict_class,0)\n",
    "        current_predict_probability = np.vstack([current_predict_probability,[0,0]])\n",
    "    current_predict_probability = np.amax(current_predict_probability,axis=1,keepdims=True)\n",
    "\n",
    "    # Testing Model Performance\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy Test : {accuracy_score(y_test,y_pred)}\")\n",
    "    print(f\"speed code is {round((b-a).microseconds * 0.001, 4)}ms\")\n",
    "    return current_predict_class,current_predict_probability\n",
    "\n",
    "def predict_future(dataframe:pd.DataFrame,training_params):\n",
    "    df = dataframe.copy()\n",
    "    global signal_predictions\n",
    "    signal_predictions = {}\n",
    "    filter_method = training_params['filter_method']\n",
    "\n",
    "    # df['predicted_value'] = df.apply((lambda x : train_model(x.name,df,training_params)),axis=1)\n",
    "    df['predicted_value'], df['predicted_proba'] = train_model(df,training_params)\n",
    "    # df['predicted_value'] = df.apply(lambda x : setPredictionAsClearWay(x.name,df,filter_method),axis=1)\n",
    "    df['isDifferentSignalType'] = fractalFilters(df['predicted_value'])\n",
    "    dataframe['predicted_value'] = df['predicted_value']\n",
    "    dataframe['predicted_proba'] = df['predicted_proba']\n",
    "    dataframe['buy_signal'] = (df['predicted_value'] > 0) & (df['isDifferentSignalType'])\n",
    "    dataframe['sell_signal'] = (df['predicted_value'] < 0) & (df['isDifferentSignalType'])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def mlRunModel(dataframe,training_params):\n",
    "    df = dataframe.copy()\n",
    "    global dataframe_for_compare\n",
    "    # heikin_ashi_dataframe = heikinashi(df)\n",
    "\n",
    "    # trend_periods = training_params['filter_params']['trend']['periods']\n",
    "    # stc_length = training_params['filter_params']['stc']['length']\n",
    "    # stc_fast_length = training_params['filter_params']['stc']['fast']\n",
    "    # stc_slow_length = training_params['filter_params']['stc']['slow']\n",
    "    # ut_sensitivity = training_params['filter_params']['ut']['sensitivity']\n",
    "    # ut_atrperiod = training_params['filter_params']['ut']['atr_period']\n",
    "\n",
    "    df = extract_features(df,training_params)\n",
    "\n",
    "    # df[Filter.volatility.value] = filter_volatility(df,1,10)\n",
    "    # df[Filter.regime.value] = regime_filter(df,training_params['filter_params']['regime']['threshold'])\n",
    "    # df[Filter.trend.value] = adaptiveTrendFinder(heikin_ashi_dataframe,trend_periods)\n",
    "    # df[Filter.stc.value] = calculateSTCIndicator(df,stc_length,stc_fast_length,stc_slow_length)\n",
    "    # df[Filter.ut.value] = calculate_ut_bot(heikin_ashi_dataframe,ut_sensitivity,ut_atrperiod)\n",
    "    # df = predict_future(df,training_params)\n",
    "    return df\n",
    "\n",
    "distances = []\n",
    "predictions = []\n",
    "signal_predictions = {}\n",
    "training_params = {\n",
    "    \"filter_method\" : [], #done\n",
    "    \"filter_params\" : {\n",
    "        \"regime\" : {\n",
    "            \"threshold\" : -0.1\n",
    "        },\n",
    "        \"trend\":{\n",
    "            \"periods\" : [0,5,10]\n",
    "        },\n",
    "        \"stc\":{\n",
    "            \"length\" : 12,\n",
    "            \"fast\" : 26,\n",
    "            \"slow\" : 50\n",
    "        },\n",
    "        \"ut\" : {\n",
    "            \"sensitivity\" : 1,\n",
    "            \"atr_period\" : 10,\n",
    "        }\n",
    "    },\n",
    "    \"is_pca_on\" : True,\n",
    "    \"neighbor_count\" : 8,\n",
    "    \"feature_count\" : 8,\n",
    "    \"future_count\" : 1, # 2 means 2 ahead if timeframe = 5m , 5*2 = 10m,if current is 30,then prediction will be 40-45\n",
    "    \"pivot_params\" : {\n",
    "        \"depth\" : 8,\n",
    "        \"deviation\" : 2,\n",
    "        \"backstep\" : 2,\n",
    "    },\n",
    "    \"f1\" : {\n",
    "        \"name\" : FeatureName.rsi,\n",
    "        \"paramsA\" : 9,\n",
    "        \"paramsB\" : 2\n",
    "    },\n",
    "    \"f2\" : {\n",
    "        \"name\" : FeatureName.rsi,\n",
    "        \"paramsA\" : 14,\n",
    "        \"paramsB\" : 2\n",
    "    },\n",
    "    \"f3\" : {\n",
    "        \"name\" : FeatureName.cci,\n",
    "        \"paramsA\" : 20,\n",
    "        \"paramsB\" : 2\n",
    "    },\n",
    "    \"f4\" : {\n",
    "        \"name\" : FeatureName.adx,\n",
    "        \"paramsA\" : 20,\n",
    "        \"paramsB\" : 2\n",
    "    },\n",
    "    \"f5\" : {\n",
    "        \"name\" : FeatureName.wt,\n",
    "        \"paramsA\" : 10,\n",
    "        \"paramsB\" : 11\n",
    "    },\n",
    "    \"f6\" : {\n",
    "        \"name\" : FeatureName.trend,\n",
    "        \"paramsA\" : [0,3,4,10,15],\n",
    "        \"paramsB\" : 0\n",
    "    },\n",
    "    \"f7\" : {\n",
    "        \"name\" : FeatureName.sma,\n",
    "        \"paramsA\" : 11,\n",
    "        \"paramsB\" : 2\n",
    "    },\n",
    "    \"f8\" : {\n",
    "        \"name\" : FeatureName.stc,\n",
    "        \"paramsA\" : \"5-10\",\n",
    "        \"paramsB\" : 2\n",
    "    }\n",
    "}\n",
    "\n",
    "a = datetime.now()\n",
    "# ml_dataframe = dataframe.copy()\n",
    "# ml_dataframe.drop(ml_dataframe.tail(1).index, inplace=True)\n",
    "complete_df = await mlRunModel(dataframe,training_params)\n",
    "# print(complete_df.tail(20).loc[:,['date','y_train','close']])\n",
    "# n_predict_candle = training_params['future_count']\n",
    "# next_predict_df = complete_df[len(complete_df)-n_predict_candle-1:len(complete_df)-1]\n",
    "# prediction_for_next_mearket = split_predictions_with_proba(next_predict_df['predicted_value'],next_predict_df['predicted_proba'],next_predict_df,n_predict_candle)\n",
    "# filtered_df = complete_df[(complete_df['buy_signal'] == True) | (complete_df['sell_signal'] == True)]\n",
    "b = datetime.now()\n",
    "# dates_with_signals = filtered_df.loc[:,['date','buy_signal','sell_signal']]\n",
    "# print(prediction_for_next_mearket)\n",
    "print(f\"speed is {round((b-a).microseconds * 0.001, 4)}ms\")\n",
    "# print(complete_df['buy_signal'].value_counts(),complete_df['sell_signal'].value_counts())\n",
    "# print(dates_wit  h_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for i in range(len(X_test)):\n",
    "            # distances = np.sum(np.log(1+np.abs(self.X_train - X_test[i])),axis=1)\n",
    "            distances = np.sqrt(np.sum(np.square(self.X_train - X_test[i]), axis=1))\n",
    "            nearest_neighbors = np.argsort(distances)[:self.k]\n",
    "            nearest_labels = [self.y_train[idx] for idx in nearest_neighbors]\n",
    "            prediction = np.sum(nearest_labels)\n",
    "            if prediction < -50:\n",
    "                prediction = -1\n",
    "            elif prediction > 50:\n",
    "                prediction = 1\n",
    "            else:\n",
    "                prediction = 0\n",
    "            # prediction = max(set(nearest_labels), key=nearest_labels.count)\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Example usage:\n",
    "a = datetime.now()\n",
    "feature_count = training_params[\"feature_count\"]\n",
    "future_count = training_params[\"future_count\"]\n",
    "is_pca_on = training_params[\"is_pca_on\"]\n",
    "\n",
    "scaler_for_train = StandardScaler()\n",
    "feature_list = [f'f{i}' for i in range(1, feature_count + 1)]\n",
    "knn_dataframe = complete_df.copy()\n",
    "\n",
    "# Data Preparation\n",
    "data_x_test = knn_dataframe[feature_list].values\n",
    "data_x_test = scaler_for_train.fit_transform(data_x_test)\n",
    "data_y_test = knn_dataframe['y_train'].values\n",
    "data_y_test_reshaped = data_y_test.reshape(-1, 1)\n",
    "combination_data = np.hstack((data_x_test, data_y_test_reshaped))\n",
    "combination_data = combination_data[~np.isnan(combination_data).any(axis=1)]\n",
    "data_x_test = combination_data[:, :-1]\n",
    "data_y_test = combination_data[:, -1]\n",
    "pca = PCA(n_components=1)\n",
    "if(is_pca_on):\n",
    "    data_x_test = pca.fit_transform(data_x_test)\n",
    "# print(knn_dataframe.tail(30).loc[:,['date','y_train','close']])\n",
    "\n",
    "# # Data Split \n",
    "train_size = 100\n",
    "\n",
    "# data_y_test_reshaped = data_y_test.reshape(-1, 1)\n",
    "# combination_data = np.hstack((data_x_test, data_y_test_reshaped))\n",
    "# temp_df = pd.DataFrame(combination_data,columns=feature_list+['y_train'])\n",
    "# temp_df.dropna(inplace=True)\n",
    "# data_x = temp_df[feature_list].values\n",
    "# data_y = temp_df['y_train'].values\n",
    "\n",
    "X_train = data_x_test[:len(data_x_test)-train_size]\n",
    "y_train = data_y_test[:len(data_y_test)-train_size]\n",
    "\n",
    "X_test = data_x_test[len(data_x_test)-train_size:]\n",
    "y_test = data_y_test[len(data_x_test)-train_size:]\n",
    "\n",
    "# Fit the model\n",
    "knn = KNN(k=100)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# knn = CatBoostClassifier(verbose=False,n_estimators=2,learning_rate=0.001)\n",
    "# knn.fit(X_train,y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "nan_index_of_y_test = np.where(np.isnan(y_test))[0]\n",
    "\n",
    "index_of_0_in_y_pred = np.where(y_pred == 0)[0]\n",
    "y_pred = np.delete(y_pred, index_of_0_in_y_pred)\n",
    "y_test = np.delete(y_test,index_of_0_in_y_pred)\n",
    "if(len(nan_index_of_y_test) > 0):\n",
    "    # print(\"Next Market Move : \",y_pred[nan_index_of_y_test[0]:])\n",
    "    b = datetime.now()\n",
    "    print(f\"speed is {round((b-a).microseconds * 0.001, 4)}ms\")\n",
    "    print(f\"Accuracy Test : {accuracy_score(y_pred[:nan_index_of_y_test[0]],y_test[:nan_index_of_y_test[0]])}\")\n",
    "else:\n",
    "    # print(\"Next Market Move : \",y_pred)\n",
    "    b = datetime.now()\n",
    "    print(len(y_test))\n",
    "    print(y_pred)\n",
    "    # print(index_of_0_in_y_pred)\n",
    "    print(f\"speed is {round((b-a).microseconds * 0.001, 4)}ms\")\n",
    "    print(f\"Accuracy Test : {accuracy_score(y_pred,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# For PCA\n",
    "# data_x_plot = data_x_test\n",
    "# data_y_plot = data_y_test\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# x_pca_1 = data_x_plot[data_y_plot == 1][:,0]\n",
    "# y_pca_1 = data_x_plot[data_y_plot == 1][:,1]\n",
    "# z_pca_1 = data_x_plot[data_y_plot == 1][:,2]\n",
    "\n",
    "# x_pca_2 = data_x_plot[data_y_plot == -1][:,0]\n",
    "# y_pca_2 = data_x_plot[data_y_plot == -1][:,1]\n",
    "# z_pca_2 = data_x_plot[data_y_plot == -1][:,2]\n",
    "# # Scatter plot\n",
    "# ax.scatter(x_pca_1,y_pca_1, z_pca_1, c='green', marker='o', alpha=0.5)\n",
    "# ax.scatter(x_pca_2,y_pca_2, z_pca_2, c='red', marker='o', alpha=0.5)\n",
    "# # Set labels and title\n",
    "# ax.set_xlabel('PCA Component 1')\n",
    "# ax.set_ylabel('PCA Component 2')\n",
    "# ax.set_zlabel('PCA Component 3')\n",
    "# ax.set_title('PCA Visualization of Training Data')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# Create trace for each class\n",
    "# trace1 = go.Scatter3d(\n",
    "#     x=X_train[y_train == 1][:, 0],\n",
    "#     y=X_train[y_train == 1][:, 1],\n",
    "#     z=X_train[y_train == 1][:, 2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=5,\n",
    "#         color='green',  # set color to an array/list of desired values\n",
    "#         opacity=0.5\n",
    "#     ),\n",
    "#     name='Class 1'\n",
    "# )\n",
    "\n",
    "# trace2 = go.Scatter3d(\n",
    "#     x=X_train[y_train == -1][:, 0],\n",
    "#     y=X_train[y_train == -1][:, 1],\n",
    "#     z=X_train[y_train == -1][:, 2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=5,\n",
    "#         color='red',  # set color to an array/list of desired values\n",
    "#         opacity=0.5\n",
    "#     ),\n",
    "#     name='Class -1'\n",
    "# )\n",
    "\n",
    "# # Create layout\n",
    "# layout = go.Layout(\n",
    "#     scene=dict(\n",
    "#         xaxis=dict(title='PCA Component 1'),\n",
    "#         yaxis=dict(title='PCA Component 2'),\n",
    "#         zaxis=dict(title='PCA Component 3')\n",
    "#     ),\n",
    "#     title='PCA Visualization of Training Data'\n",
    "# )\n",
    "\n",
    "# # Create figure\n",
    "# fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "\n",
    "# # Show plot\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
